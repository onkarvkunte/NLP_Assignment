{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "M2_Assignment_Part_I.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onkarvkunte/NLP_Assignment/blob/main/scripts/Part%20I.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBtJv0BsJxNx"
      },
      "source": [
        "##Part I:  Vector Semantics and Motivation for Word Embeddings\n",
        "\n",
        "It is important to understand the words meaning (recall semantics) AND their context. Words that are seen in the similar context also often have similar meaning. The distributional hypothesis expresses this phenomenon saying that there is a link in similarity in how words are distributed and their likeness.  Vector semantics is the concept of learning representations of meanings of words – called embeddings—from their distributions in a corpus or corpora. Fundamentally, we are asking the question with NLP: how might we represent the meaning of a word and interpret it?\n",
        "\n",
        "A word embedding is simply a to represent words in a numerical context -- a vector.  This is important because Neural Networks and Machine Learning models don't learn on the text itself, but the numerical representation of the text. In fact, there is typically an \"embedding layer\" as part of the simplest NLP-based neural networks as you will find.\n",
        "\n",
        "The simplest way to show this is called a one-hot vector, other forms include term frequencies of words (as we have seen with Bayesian models), Term Frequency-Inverse Document Frequency, which normalizes terms across documents, and distributional representations, which are context-based encodings that help derive similarity-- i.e., \"queen is to female as king is to male\".\n",
        "\n",
        "We will start simple and discuss some of the challenges, then move to more complex transformations.\n",
        "\n",
        "## Setup\n",
        "As part of completing the assignment, you will see that there are areas in the note book for you to complete your own coding input.\n",
        "\n",
        "It will be look like following:\n",
        "```\n",
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "'Some coding activity for you to complete'\n",
        "### END CODE HERE ###\n",
        "\n",
        "```\n",
        "Please be sure to fill these code snippets out as you turn in your assignment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKPWU-Ip2AZL"
      },
      "source": [
        "##1.1 One-hot vector\n",
        "\n",
        "A one-hot vector helps to translate categorical or sequential data to something that is machine readable and also does not have an impact on your model. Each word in the sequence is given a binary encoding and is mapped to a vector of the length of the the input. This is a common pre-processing step for the input layer in a neural network.\n",
        "\n",
        "One hot encoding assigns a unique code for each unique word. As an example, we can take the following sentence and convert it to a one-hot vector.\n",
        "\n",
        "\"Live as if you were to die tomorrow. Learn as if you were to live forever\"\n",
        "\n",
        "We will use NLTK to tokenize the sentence, then Sci-Kit Learn to apply the one-hot encoder. Note, that SK-Learn will apply a single value for a unique word in a vector which is great for categorical representations. This one-hot encoding has traditionally been used for feeding categorical data to many scikit-learn estimators in shallow learning models such as notably linear models and SVMs with the standard kernels.\n",
        "\n",
        "Note: This approach is inefficient. A one-hot encoded vector is sparse (meaning, most indices are zero). Imagine you have 10,000 words in the vocabulary. To one-hot encode each word, you would create a vector where 99.99% of the elements are zero.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 692
        },
        "id": "V5pWPsfD2BfJ",
        "outputId": "327873b1-b177-43cf-b918-0119949eea36"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from numpy import argmax\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "nltk.download('punkt')\n",
        "\n",
        "#%matplotlib inline\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# define input string\n",
        "data = 'Live as if you were to die tomorrow. Learn as if you were to live forever'\n",
        "#tokenize that string\n",
        "wordlist = nltk.word_tokenize(data.lower())\n",
        "#create a vector representation of the wordlist\n",
        "wordlist_clean = []\n",
        "\n",
        "for i in wordlist: # Go through every word in your tokens list\n",
        "    if (i not in string.punctuation):  # remove punctuation\n",
        "        wordlist_clean.append(i)\n",
        "# define universe of possible input values\n",
        "wordlist_clean_df = pd.DataFrame(data=wordlist_clean, columns=['words'])\n",
        "\n",
        "#encode using scki-kit learn\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "one_hot_encoder.fit(wordlist_clean_df)\n",
        "wordlist_clean_df_encoded = one_hot_encoder.transform(wordlist_clean_df)\n",
        "wordlist_clean_df_encoded = pd.DataFrame(data=wordlist_clean_df_encoded, columns=one_hot_encoder.categories_)\n",
        "print('\\n\\n One-Hot Encoded Vector using SKLearn')\n",
        "display(wordlist_clean_df_encoded)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            " One-Hot Encoded Vector using SKLearn\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "     as  die forever   if learn live   to tomorrow were  you\n",
              "0   0.0  0.0     0.0  0.0   0.0  1.0  0.0      0.0  0.0  0.0\n",
              "1   1.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  0.0\n",
              "2   0.0  0.0     0.0  1.0   0.0  0.0  0.0      0.0  0.0  0.0\n",
              "3   0.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  1.0\n",
              "4   0.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  1.0  0.0\n",
              "5   0.0  0.0     0.0  0.0   0.0  0.0  1.0      0.0  0.0  0.0\n",
              "6   0.0  1.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  0.0\n",
              "7   0.0  0.0     0.0  0.0   0.0  0.0  0.0      1.0  0.0  0.0\n",
              "8   0.0  0.0     0.0  0.0   1.0  0.0  0.0      0.0  0.0  0.0\n",
              "9   1.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  0.0\n",
              "10  0.0  0.0     0.0  1.0   0.0  0.0  0.0      0.0  0.0  0.0\n",
              "11  0.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  0.0  1.0\n",
              "12  0.0  0.0     0.0  0.0   0.0  0.0  0.0      0.0  1.0  0.0\n",
              "13  0.0  0.0     0.0  0.0   0.0  0.0  1.0      0.0  0.0  0.0\n",
              "14  0.0  0.0     0.0  0.0   0.0  1.0  0.0      0.0  0.0  0.0\n",
              "15  0.0  0.0     1.0  0.0   0.0  0.0  0.0      0.0  0.0  0.0"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-deb3e9b9-5414-4c09-9edb-eb7852a29926\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>as</th>\n",
              "      <th>die</th>\n",
              "      <th>forever</th>\n",
              "      <th>if</th>\n",
              "      <th>learn</th>\n",
              "      <th>live</th>\n",
              "      <th>to</th>\n",
              "      <th>tomorrow</th>\n",
              "      <th>were</th>\n",
              "      <th>you</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-deb3e9b9-5414-4c09-9edb-eb7852a29926')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-c18364fa-db19-461b-b612-9313745a2e24\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c18364fa-db19-461b-b612-9313745a2e24')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-c18364fa-db19-461b-b612-9313745a2e24 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-deb3e9b9-5414-4c09-9edb-eb7852a29926 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-deb3e9b9-5414-4c09-9edb-eb7852a29926');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTRMr2m-oxRO"
      },
      "source": [
        "##1.2 Encoding as a dense  - Singular Value Decomposition\n",
        "A second approach you might try is to encode each word using a unique number. This helps with reducing dimensionality and attempts to address the problem of very large sparse matrices. Continuing the example above, you could assign 1 to \"live\", 2 to \"the\", and so on. You could then encode the sentence \"The cat sat on the mat\" as a dense vector. Now, instead of a sparse vector, you now have a dense one. A dense vector is a vector where all elements are populated with a non-zero value.\n",
        "\n",
        "There are several challenges:\n",
        "\n",
        "1.   The integer-encoding is arbitrary (it does not capture any relationship between words)\n",
        "2.   An integer-encoding can be challenging for a model to interpret. A linear classifier, for example, learns a single weight for each feature. Because there is no relationship between the similarity of any two words and the similarity of their encodings, this feature-weight combination is not meaningful.\n",
        "3.  Word order is ignored.\n",
        "4.  Raw absolute frequency counts of words do not necessarily represent the meaning of the text properly\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "0C-2Kg9Dq05v",
        "outputId": "a4e6135f-9a29-471e-c513-8e6d96f29635"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "from numpy import argmax\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "nltk.download('punkt')\n",
        "\"\"\"## Default Style Settings\n",
        "matplotlib.rcParams['figure.dpi'] = 150\n",
        "pd.options.display.max_colwidth = 200\n",
        "#%matplotlib inline\"\"\"\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# define input string\n",
        "data = 'Live as if you were to die tomorrow. Learn as if you were to live forever'\n",
        "#tokenize that string\n",
        "wordlist = nltk.word_tokenize(data.lower())\n",
        "#create a vector representation of the wordlist\n",
        "wordlist_clean = []\n",
        "\n",
        "for i in wordlist: # Go through every word in your tokens list\n",
        "    if (i not in string.punctuation):  # remove punctuation\n",
        "        wordlist_clean.append(i)\n",
        "# define universe of possible input values\n",
        "wordlist_clean_df = pd.DataFrame(data=wordlist_clean, columns=['words'])\n",
        "dense_vector = np.unique(wordlist_clean_df, return_counts=True)\n",
        "dense_vector_df = pd.DataFrame(data=dense_vector, columns = np.unique(wordlist_clean_df))\n",
        "display(dense_vector_df)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   as  die  forever  if  learn  live  to  tomorrow  were  you\n",
              "0  as  die  forever  if  learn  live  to  tomorrow  were  you\n",
              "1   2    1        1   2      1     2   2         1     2    2"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-b728e15d-4c47-43fb-b52d-c2f7d12cefd5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>as</th>\n",
              "      <th>die</th>\n",
              "      <th>forever</th>\n",
              "      <th>if</th>\n",
              "      <th>learn</th>\n",
              "      <th>live</th>\n",
              "      <th>to</th>\n",
              "      <th>tomorrow</th>\n",
              "      <th>were</th>\n",
              "      <th>you</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>as</td>\n",
              "      <td>die</td>\n",
              "      <td>forever</td>\n",
              "      <td>if</td>\n",
              "      <td>learn</td>\n",
              "      <td>live</td>\n",
              "      <td>to</td>\n",
              "      <td>tomorrow</td>\n",
              "      <td>were</td>\n",
              "      <td>you</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b728e15d-4c47-43fb-b52d-c2f7d12cefd5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-f029adb1-cd18-4998-b1b2-eefc0402e485\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f029adb1-cd18-4998-b1b2-eefc0402e485')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-f029adb1-cd18-4998-b1b2-eefc0402e485 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b728e15d-4c47-43fb-b52d-c2f7d12cefd5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b728e15d-4c47-43fb-b52d-c2f7d12cefd5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5217etS2QW5"
      },
      "source": [
        "##1.3 Text Vectorization\n",
        "\n",
        "\n",
        "*   Overview\n",
        "*   N-Gram Bag of Words\n",
        "*   Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "*   Document Similarity: Cosine Similarity, Jaccard Similarity, Euclidian Similarity\n",
        "*   Topic Modeling Exercise\n",
        "\n",
        "### Why do we do it?\n",
        "These subsquent categories of text vectorizations are ways to derive similarity from text documents. This is useful for NLP tasks such as topic modeling -- where we aim to show the relationship between documents via a category or topic. You will see how TF-IDF can be used to support topic modeling.\n",
        "\n",
        "Here are some text vectorization approaches in summary:\n",
        "![Text Vectorization Approaches](https://drive.google.com/uc?export=view&id=12GYWDaK5_offSn3Gy-hv_KpuTc4A_mGA)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKq4TXKo1zvi"
      },
      "source": [
        "\n",
        "### 1.3.1 N-Gram Bag-Of Words Model\n",
        "You've already learned the bag-of words model above with one-hot encoding and dense vectorization! We are counting the frequencies of words in the matrix in a dense representation of the word vector. What happens if we took some steps to improve the Bag-of-Words model by incorporating the n-gram approach we have learned earlier in the class.\n",
        "\n",
        "What does this do?\n",
        "If our goal is to identify words in texts that represent meaning of that text, then recall that taking the bi-gram, tri-gram, or n-gram of a corpus allows us to bring in context via the word order. With a simple BOW approach, no word order is considers. Moreover, we can filter words based on distributional counts -- that is, term frequencies. Imagine that the counts of a word fall into say a Gaussian (normal) Distribution across a number of corpora. We can use the distribution to filter out salient word-phrases or sequences in which we can infer the meaning of the text. Finally, we can apply weights to the frequency counts -- similar to weight vector in a NN-- in which those weights have an impact on word relationships or salience.\n",
        "\n",
        "\n",
        "![Bag of Words](https://drive.google.com/uc?export=view&id=1btCVz_8JWYTvE73qGLCRZb7nXg-kCiDU)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gR43ePKDKlu"
      },
      "source": [
        "#### 1.3.1.2 Example: N-Gram Bag-Of Words Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 477
        },
        "id": "cv1H2Ha-0RMP",
        "outputId": "3b260fe4-41e1-47b9-95e9-83d7d73921b7"
      },
      "source": [
        "#example from: https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/exercise/5-text-vectorization.html#\n",
        "\n",
        "import pandas as pd                        # Python library for pandas - data maniplation\n",
        "import numpy as np                         # Python library for numpy -- matrix algebra library\n",
        "import matplotlib                          # Python library for matplotlib -- visual display of data\n",
        "import matplotlib.pyplot as plt            # Python library for matplotlib -- visual display of data\n",
        "import nltk                                # Python library for NLP\n",
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "nltk.download('stopwords')                 # package for stop words\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "## Default Style Settings\n",
        "matplotlib.rcParams['figure.dpi'] = 150\n",
        "pd.options.display.max_colwidth = 200\n",
        "#%matplotlib inline\n",
        "\n",
        "corpus = [\n",
        "    'The sky is blue and beautiful.', 'Love this blue and beautiful sky!',\n",
        "    'The quick brown fox jumps over the lazy dog.',\n",
        "    \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\",\n",
        "    'I love green eggs, ham, sausages and bacon!',\n",
        "    'The brown fox is quick and the blue dog is lazy!',\n",
        "    'The sky is very blue and the sky is very beautiful today',\n",
        "    'The dog is lazy but the brown fox is quick!'\n",
        "]\n",
        "labels = [\n",
        "    'weather', 'weather', 'animals', 'food', 'food', 'animals', 'weather',\n",
        "    'animals'\n",
        "]\n",
        "\n",
        "corpus = np.array(corpus) # np.array better than list\n",
        "corpus_df = pd.DataFrame({'Document': corpus, 'Category': labels})\n",
        "corpus_df\n",
        "\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I | re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = wpt.tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)\n",
        "\n",
        "norm_corpus = normalize_corpus(corpus)\n",
        "print(corpus)\n",
        "print(\"=\"*50)\n",
        "print(norm_corpus)\n",
        "\n",
        "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n",
        "bv = CountVectorizer(ngram_range=(2, 2))\n",
        "bv_matrix = bv.fit_transform(norm_corpus)\n",
        "\n",
        "bv_matrix = bv_matrix.toarray()\n",
        "vocab = bv.get_feature_names()\n",
        "pd.DataFrame(bv_matrix, columns=vocab)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The sky is blue and beautiful.' 'Love this blue and beautiful sky!'\n",
            " 'The quick brown fox jumps over the lazy dog.'\n",
            " \"A king's breakfast has sausages, ham, bacon, eggs, toast and beans\"\n",
            " 'I love green eggs, ham, sausages and bacon!'\n",
            " 'The brown fox is quick and the blue dog is lazy!'\n",
            " 'The sky is very blue and the sky is very beautiful today'\n",
            " 'The dog is lazy but the brown fox is quick!']\n",
            "==================================================\n",
            "['sky blue beautiful' 'love blue beautiful sky'\n",
            " 'quick brown fox jumps lazy dog'\n",
            " 'kings breakfast sausages ham bacon eggs toast beans'\n",
            " 'love green eggs ham sausages bacon' 'brown fox quick blue dog lazy'\n",
            " 'sky blue sky beautiful today' 'dog lazy brown fox quick']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-6e2dc4f6ac56>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mbv_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbv_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbv_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CountVectorizer' object has no attribute 'get_feature_names'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Be0ML9kADimc"
      },
      "source": [
        "### 1.3.2 Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "As an extension of the BOW model, we can weight the frequency (counts) of the terms in a document by considering its *dispersion*. Fundamentally, we are taken the total frequency of a word and dividing it by the number of documents with that term -- this gives us term frequency.\n",
        "\n",
        "\n",
        "Then we take the inverse The formula for TF-IDF will look something like:\n",
        "\n",
        "\n",
        "*   Term Frequency(TF): the number of times a word appears in a document. These are the raw absolute frequency counts of the words in the BOW model.\n",
        "*   Inverse Document Frequency(IDF): total documents in corpus over number of documents with term.\n",
        "\n",
        "> $\\textit{TF-IDF} = {tf \\times idf}$\n",
        "\n",
        "Here, the general idea is that we can extropolate the meaningful words from a corpus by inversing their frequency. For example, \"The\" in the corpus may be frequently observed, but does not garner meaning. We can use this for keyword extraction, and information retrieval tasks.\n",
        "\n",
        "Let's normalize this function to account for divide-by-zero erros and to also smooth the weighting scheme.\n",
        "\n",
        "Addressing divide-by-zero errors. Similar to Laplace Smoothing techniques, we will typically add one to the IDF formula:\n",
        "\n",
        "> $\\textit{IDF} = 1 + log\\frac{N}{1+df}$\n",
        "\n",
        "We also might normalize the final IF-IDF function using an L2 Norm (see more in Jurafsky, Chapter 6).\n",
        "\n",
        "> $\\textit{TF-IDF}_{normalized} = \\frac{tf \\times idf}{\\sqrt{(tf\\times idf)^2}}$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4tJ1Yrbbufr"
      },
      "source": [
        "#### 1.3.2.1 Example: TF-IDF Usage\n",
        "In our example, we use the TfidfTransformer function to apply L2 norms and smoothing techniques.\n",
        "\n",
        "```\n",
        "tt = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True)\n",
        "```\n",
        "\n",
        "\n",
        "Let's use the same corpus from above in our example for TD-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KROXdPWWRlv"
      },
      "source": [
        "norm_corpus = ['sky blue beautiful', 'love blue beautiful sky',\n",
        " 'quick brown fox jumps lazy dog',\n",
        " 'kings breakfast sausages ham bacon eggs toast beans',\n",
        " 'love green eggs ham sausages bacon', 'brown fox quick blue dog lazy',\n",
        " 'sky blue sky beautiful today' ,'dog lazy brown fox quick']\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# get bag of words features in sparse format\n",
        "cv = CountVectorizer(min_df=0., max_df=1.)\n",
        "cv_matrix = cv.fit_transform(norm_corpus)\n",
        "cv_matrix\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "\n",
        "\"\"\"Note: With Tfidftransformer you will systematically compute word counts using CountVectorizer\n",
        "and then compute the Inverse Document Frequency (IDF) values and only then compute the Tf-idf scores.\"\"\"\n",
        "tt = TfidfTransformer(norm='l2',\n",
        "                      use_idf=True,\n",
        "                      smooth_idf=True)\n",
        "tt_matrix = tt.fit_transform(cv_matrix)\n",
        "tt_matrix = tt_matrix.toarray()\n",
        "vocab = cv.get_feature_names()\n",
        "tt_df = pd.DataFrame(np.round(tt_matrix, 2), columns=vocab)\n",
        "display(tt_df)\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"Note: WWith Tfidfvectorizer on the contrary, you will do all three steps at once.\n",
        "Under the hood, it computes the word counts, IDF values, and Tf-idf scores all using the same dataset.\"\"\"\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tv = TfidfVectorizer(min_df=0.,\n",
        "                     max_df=1.,\n",
        "                     norm='l2',\n",
        "                     use_idf=True,\n",
        "                     smooth_idf=True)\n",
        "tv_matrix = tv.fit_transform(norm_corpus)\n",
        "tv_matrix = tv_matrix.toarray()\n",
        "\n",
        "vocab = tv.get_feature_names()\n",
        "tv_df  = pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)\n",
        "display(tv_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfXbJjDdV5V4"
      },
      "source": [
        "### 1.3.4 Document Similarity and Word Semantics\n",
        "\n",
        "Lexical semantics is a branch of linguistics focused on meaning and word relationships. Moreover, the idea behind word sense is the interpretation of the word (often requiring context to understand). Where multiple meanings can occur for a word – take the example of a mouse that can mean both the cursor controller and the rodent—we must discern using context. Relationships between word senses can be referred to as synonyms (e.g., couch/sofa).\n",
        "\n",
        "**Word Similarity** is not the same as a synonym, rather it is the idea that words have relationships. The example of cat and dog is used to show that while they are not synonymous, they are both animals, often they are domesticated – their semantics are similar.\n",
        "\n",
        "**Word relatedness** is slightly different than word similarity where there is more of a psychological association—for example, that coffee and cup are related.\n",
        "Recall that vectors for representing words are called embeddings implying that a point in space can be mapped to another point in space.\n",
        "\n",
        "This is important because word similarity (measured through a vector representing distance between two words in space) can be powerful for tasks we have previously done, such as sentiment analysis. Moreover, we can derive the meaning of the word using the nearby counts of similar words.\n",
        "\n",
        "We look at three similarity metrics to score word and/or document similarity:\n",
        "\n",
        "*   Manhattan Distance: is the sum of absolute differences between points across all the dimensions. Called \"Manhattan\" because we can think of getting from point (a,b) to point (c,d) on a Cartesian plane by only travelining vertically or horizontally, not diagnally.\n",
        "*   Euclidian Distance: is the shortest distance between two points in mathmatics. Not as useful in the field of NLP. The \"as the crow flies\" distance.\n",
        "*   Cosine Similarity: measure similarity based on the content overlap between documents.\n",
        "*   Jaccard Similarity: Used to identify documents we measure it as proportion of number of common words to number of unique words in both documents.\n",
        "\n",
        "Note: Generally speaking the difference betweem *distance* and *similarity* is basically that distance is just equal to 1 - similarity.\n",
        "\n",
        "\n",
        "Let's take a look at Cosine Similarity metrics since this is most commonly used with NLP and also with word2vec.\n",
        "\n",
        "> $similarity(doc_1, doc_2) = cos(\\theta) = \\frac{doc_1  doc_2}{\\lvert doc_1\\rvert \\lvert doc_2\\rvert}$\n",
        "\n",
        "By cosine distance/dissimilarity we assume following:\n",
        "> $distance(doc_1, doc_2) = 1 - similarity(doc_1, doc_2)$\n",
        "\n",
        "The similarity-based metics look like the following🇰\n",
        "> cos(\\vec{x},\\vec{y}) = \\frac{\\sum_{i=1}^{n}{x_i\\times y_i}}{\\sqrt{\\sum_{i=1}^{n}x_i^2}\\times \\sqrt{\\sum_{i=1}^{n}y_i^2}}\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "cosine_similarity(xyz)\n",
        "array([[1.        , 0.97780241, 0.30320366],\n",
        "       [0.97780241, 1.        , 0.49613894],\n",
        "       [0.30320366, 0.49613894, 1.        ]])\n",
        "```\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1c9D33toCdC1W3_SRiUawykcspho8IVfI)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBFAAHro4KyS"
      },
      "source": [
        "### **1.3.5: Exercise: BOW with n-gram**\n",
        "Use the *brown* corpus to create a n-gram BOW model. First, you must clean and organize the data. Then enter your code to complete the exercise.\n",
        "\n",
        "The Brown Corpus is an collection of text samples of American English categorized by various genres such as science-fiction, adventure, etc.\n",
        "\n",
        "Create a tri-gram bag of words matrix using the brown corpus as its inputs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrrT-i-u4_7M"
      },
      "source": [
        "import pandas as pd                        # Python library for pandas - data maniplation\n",
        "import numpy as np                         # Python library for numpy -- matrix algebra library\n",
        "import matplotlib                          # Python library for matplotlib -- visual display of data\n",
        "import matplotlib.pyplot as plt            # Python library for matplotlib -- visual display of data\n",
        "import nltk                                # Python library for NLP\n",
        "import re                                  # library for regular expression operations\n",
        "import string                              # for string operations\n",
        "\n",
        "nltk.download('stopwords')                 # package for stop words\n",
        "nltk.download('brown')                 # package for stop words\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "from nltk.corpus import brown              # this is the corpus you use for this exercise.\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#The seed() method is used to initialize the random number generator\n",
        "np.random.seed(100)\n",
        "\n",
        "brown_cat= brown.categories() # Creates a list of categories\n",
        "\n",
        "docs=[]\n",
        "for cat in brown_cat: # We append tuples of each document and categories in a list\n",
        "    t1=brown.sents(categories=cat) # At each iteration we retrieve all documents of a given category\n",
        "    for doc in t1:\n",
        "        docs.append((' '.join(doc), cat)) # These documents are appended as a tuple (document, category) in the list\n",
        "\n",
        "brown_df=pd.DataFrame(docs, columns=['sentence', 'category']) #The data frame is created using the generated tuple.\n",
        "\n",
        "brown_df.head()\n",
        "\n",
        "\n",
        "#Step 1. Pre-Processing the Brown Corpus Text\n",
        "wpt = nltk.WordPunctTokenizer()\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I | re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = wpt.tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc\n",
        "\n",
        "normalize_corpus = np.vectorize(normalize_document)\n",
        "\n",
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "\n",
        "#create some normalized corpus from the pre-processing functiong above\n",
        "normalize_corpus = None\n",
        "\n",
        "#Using the nromalized corpus.\n",
        "#Because the brown corpus is very large,select 10,000 random records from the corpus. Set seed so you can return the same results.\n",
        "norm_corpus = normalize_corpus(None)\n",
        "\n",
        "#Step 2. Create a tri-gram data frame and count its frequencies\n",
        "None\n",
        "\n",
        "#print the dataframe to show the tri-gram BOW\n",
        "pd.DataFrame(bv_matrix, columns=vocab)\n",
        "\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1I-GlKzVk7B"
      },
      "source": [
        "### 1.3.6 Exercise: TD-IDF\n",
        "\n",
        "Now, using anyone of the following datasets, create you're own TF-IDF implementation. Provide your output in the form of a matrix.\n",
        "\n",
        "For more on the intuition behind TF-IDF, read the article [here](https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/text-vec-traditional.html).\n",
        "\n",
        "Refer to [this article](https://sci2lab.github.io/ml_tutorial/tfidf/) related to TF-IDF and Elastisearch. Note how the TF-IDF approach can be used for information retrieval.\n",
        "\n",
        "Datasets that you may choose from:\n",
        "*   [Reviews Dataset](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences#). this dataset uses classified data from Yelp!, Amazon, and IMBD. You can use this to determine TF-IDF across the datasets.\n",
        "\n",
        "\n",
        "*   Presidential speeches in NLTK. You can use this dataset to determine the TFIDF vector of words across presidential speeches.\n",
        "\n",
        "```\n",
        "nltk.corpus.inaugural\n",
        "```\n",
        "\n",
        "Please provide your code in the cell below.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Bckx5RzyUob"
      },
      "source": [
        "### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ###\n",
        "None\n",
        "### END CODE HERE ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEGeTp9idVUk"
      },
      "source": [
        "##A. References\n",
        "\n",
        "1.   Chapter 6 – Vector Semantics and Word Embeddings Speech and Language rocessing. Daniel Jurafsky & James H. Martin. Copyright © 2021. All rights reserved. Draft of September 21, 2021.\n",
        "2.   [Word2vec from Scratch with NumPy](https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72)\n",
        "3.   [A hands=on intutive approach to Deep Learning Methods for Text Data - Word2Vec,GloVe and FastText](https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa)\n",
        "4.    [Traditional Methods for Text Data](https://towardsdatascience.com/understanding-feature-engineering-part-3-traditional-methods-for-text-data-f6f7d70acd41)\n",
        "5.    [Word Embeddings](https://colab.research.google.com/github/tensorflow/text/blob/master/docs/guide/word_embeddings.ipynb#scrollTo=Q6mJg1g3apaz)\n",
        "6. [CS 224D: Deep Learning for NLP](https://cs224d.stanford.edu/lecture_notes/LectureNotes1.pdf)\n",
        "7. [Text Vectorization] (https://alvinntnu.github.io/NTNU_ENC2045_LECTURES/nlp/text-vec-traditional.html)\n",
        "8. [Brown Corpus](https://en.wikipedia.org/wiki/Brown_Corpus)\n",
        "9. [TF-IDF](https://ethen8181.github.io/machine-learning/clustering_old/tf_idf/tf_idf.html)\n",
        "10. [Applying TF-IDF algorithm in practice](https://plumbr.io/blog/programming/applying-tf-idf-algorithm-in-practice)\n",
        "11. [text2vec](http://text2vec.org/similarity.html)\n",
        "\n"
      ]
    }
  ]
}